{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Example code for various LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Databricks Dolly\n",
    "git repo: https://github.com/databrickslabs/dolly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from training.generate import generate_response, InstructionTextGenerationPipeline, load_model_tokenizer_for_generate\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# general setup\n",
    "default_model = \"databricks/dolly-v2-3b\"\n",
    "\n",
    "suggested_models = [\n",
    "    \"databricks/dolly-v1-6b\",\n",
    "    \"databricks/dolly-v2-3b\",\n",
    "    \"databricks/dolly-v2-7b\",\n",
    "    \"databricks/dolly-v2-12b\",\n",
    "]\n",
    "\n",
    "dbutils.widgets.combobox(\"input_model\", default_model, suggested_models, \"input_model\")\n",
    "\n",
    "input_model = dbutils.widgets.get(\"input_model\")\n",
    "\n",
    "model, tokenizer = load_model_tokenizer_for_generate(input_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Text generation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Examples from https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html\n",
    "instructions = [\n",
    "    \"Explain to me the difference between nuclear fission and fusion.\",\n",
    "    \"Give me a list of 5 science fiction books I should read next.\",\n",
    "]\n",
    "\n",
    "# Use the model to generate responses for each of the instructions above.\n",
    "for instruction in instructions:\n",
    "    response = generate_response(instruction, model=model, tokenizer=tokenizer)\n",
    "    if response:\n",
    "        print(f\"Instruction: {instruction}\\n\\n{response}\\n\\n-----------\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Langchain (for generation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(\n",
    "    pipeline=InstructionTextGenerationPipeline(\n",
    "        # Return the full text, because this is what the HuggingFacePipeline expects.\n",
    "        model=model, tokenizer=tokenizer, return_full_text=True, task=\"text-generation\"))\n",
    "\n",
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Examples from https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html\n",
    "instructions = [\n",
    "    \"Explain to me the difference between nuclear fission and fusion.\",\n",
    "    \"Give me a list of 5 science fiction books I should read next.\",\n",
    "]\n",
    "\n",
    "# Use the model to generate responses for each of the instructions above.\n",
    "for instruction in instructions:\n",
    "    response = llm_chain.predict(instruction=instruction)\n",
    "    print(f\"Instruction: {instruction}\\n\\n{response}\\n\\n-----------\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "context = (\n",
    "    \"\"\"George Washington (February 22, 1732[b] â€“ December 14, 1799) was an American military officer, statesman, \"\"\"\n",
    "    \"\"\"and Founding Father who served as the first president of the United States from 1789 to 1797. Appointed by \"\"\"\n",
    "    \"\"\"the Continental Congress as commander of the Continental Army, Washington led Patriot forces to victory in \"\"\"\n",
    "    \"\"\"the American Revolutionary War and served as president of the Constitutional Convention of 1787, which \"\"\"\n",
    "    \"\"\"created and ratified the Constitution of the United States and the American federal government. Washington \"\"\"\n",
    "    \"\"\"has been called the \"Father of his Country\" for his manifold leadership in the nation's founding.\"\"\"\n",
    ")\n",
    "\n",
    "instruction = \"When did George Washinton serve as president of the Constitutional Convention?\"\n",
    "\n",
    "response = llm_context_chain.predict(instruction=instruction, context=context)\n",
    "print(f\"Instruction: {instruction}\\n\\nContext:\\n{context}\\n\\nResponse:\\n{response}\\n\\n-----------\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipeline (for generation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generation_pipeline = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Examples from https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html\n",
    "instructions = [\n",
    "    \"Explain to me the difference between nuclear fission and fusion.\",\n",
    "    \"Give me a list of 5 science fiction books I should read next.\",\n",
    "]\n",
    "\n",
    "# Use the model to generate responses for each of the instructions above.\n",
    "for instruction in instructions:\n",
    "    results = generation_pipeline(instruction, num_return_sequences=2)\n",
    "\n",
    "    print(f\"Instruction: {instruction}\\n\")\n",
    "    for i, res in enumerate(results, 1):\n",
    "        text = res[\"generated_text\"]\n",
    "        print(f\"Sample #{i}:\\n{text}\\n\")\n",
    "    print(\"-----------\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}